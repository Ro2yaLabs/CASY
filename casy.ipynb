{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from docx import Document\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "import elevenlabs\n",
    "import subprocess\n",
    "import os\n",
    "from typing import Iterator\n",
    "from random import randint\n",
    "\n",
    "from wav2lip_master import inference_yolo\n",
    "from dataclasses import dataclass\n",
    "from wav2lip_master import audio\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display\n",
    "\n",
    "import base64\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    checkpoint_path = r\"E:\\chat\\wav2lip_master\\checkpoints\\wav2lip.pth\"\n",
    "    audio = r\"E:\\chat\\inputs\\ms.wav\"\n",
    "    face = r\"E:\\chat\\inputs\\salma720.mp4\"\n",
    "    outfile = r'E:\\chat\\outputs\\out.mp4'\n",
    "    frame_path = r\"E:\\chat\\frames\"\n",
    "    fps = 25\n",
    "    face_det_batch_size = 16\n",
    "    wav2lip_batch_size = 32\n",
    "    resize_factor = 1\n",
    "    crop = [0, -1, 0, -1]\n",
    "    box = [-1, -1, -1, -1]\n",
    "    rotate = False\n",
    "    nosmooth = False\n",
    "    save_frames = False\n",
    "    static = False\n",
    "    save_as_video = False\n",
    "    img_size = 96\n",
    "    pads = [0, 0, 0, 0]\n",
    "    mel_step_size = 16\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    \n",
    "\n",
    "class Casy:\n",
    "    def __init__(self, file_path, i):\n",
    "\n",
    "        self.args = Args()\n",
    "\n",
    "        self.model_id = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "\n",
    "        self.wav2lib_model = inference_yolo.load_model(self.args.checkpoint_path)\n",
    "        self.yolo_model = YOLO('wav2lip_master/yolo/best.pt')\n",
    "\n",
    "        self.dim = 384\n",
    "        self.file_path = file_path\n",
    "        chroma_client = chromadb.PersistentClient(path=f\"./dp/demo{i}\")\n",
    "        self.collection = chroma_client.get_or_create_collection(\n",
    "            name=\"book\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        full_text = self.read_docx(self.file_path)\n",
    "        splitted_txt = self.splitter(full_text)\n",
    "        self.model = self._encode()\n",
    "        encoded_text = self.model.encode(splitted_txt, show_progress_bar=True).tolist()\n",
    "        ids = [str(i) for i in range(len(encoded_text))]\n",
    "        self.collection.upsert(\n",
    "            documents=splitted_txt,\n",
    "            embeddings=encoded_text,\n",
    "            ids=ids\n",
    "        )\n",
    "        self.system = \"\"\"\n",
    "                I'll provide you with a JSON object that contains a question and the context related to it:\n",
    "                {\"question\": the question, \"context\": the context}\n",
    "                Please generate the answer of the provided question based on the context above in just 40 words no more even if the answer is missing something.\n",
    "                \"\"\"\n",
    "        \n",
    "        api_key = \"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\"\n",
    "        elevenlabs.set_api_key(\"f8b8bd17f45040b85ee67d3d0c6f0b1d\")\n",
    "        \n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "        self.messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system}, \n",
    "        ]\n",
    "\n",
    "    def run(self, question):\n",
    "        question_embed = self.model.encode(question)\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=question_embed.tolist(),\n",
    "            n_results=3,  \n",
    "        )\n",
    "        top_paragraph = ' '.join([i for i in results['documents']][0])\n",
    "        prompt = '{\"question\": ' + question + ', \"context\": ' + top_paragraph + '}'\n",
    "\n",
    "        self.messages.append(\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        )\n",
    "        self.audio = b\"\"\n",
    "        self.out = cv2.VideoWriter(f\"temp/res1.avi\",\n",
    "                                        cv2.VideoWriter_fourcc(*'DIVX'), 25, (720, 720))\n",
    "        self.generate_audio(prompt, self.messages)\n",
    "        self.out.release()\n",
    "        \n",
    "        audio_segment = AudioSegment.from_file(BytesIO(self.audio), format=\"mp3\")\n",
    "        # audio_segment_resampled = audio_segment.set_frame_rate(16000)\n",
    "        audio_segment.export('temp/res.wav', format=\"wav\")\n",
    "        \n",
    "        # -c:v copy -c:a aac -strict experimental -ar 16000 -shortest -q:v 1\n",
    "        command = 'ffmpeg -y -i {} -i {} -strict -2 -q:v 1 {}'.format('temp/res.wav', 'temp/res1.avi', 'temp/res.mp4')\n",
    "        subprocess.call(command, shell=False)\n",
    "        \n",
    "    def read_docx(self, file_path):\n",
    "        doc = Document(file_path)\n",
    "        full_text = []\n",
    "        for para in doc.paragraphs:\n",
    "            full_text.append(para.text)\n",
    "        full_text = '\\n'.join(full_text)\n",
    "\n",
    "        return full_text\n",
    "\n",
    "    def splitter(self, txt):\n",
    "        \n",
    "        chunk_size = 1000\n",
    "        chunk_overlap = 200\n",
    "\n",
    "        def length_function(text: str) -> int:\n",
    "            return len(text)\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=length_function\n",
    "        )\n",
    "\n",
    "        return splitter.split_text(txt)\n",
    "    \n",
    "    def _encode(self):\n",
    "        return SentenceTransformer(self.model_id, device=self.args.device)\n",
    "    \n",
    "    def get_smoothened_boxes(self, boxes, T):\n",
    "        \"\"\"\n",
    "        Smooth the bounding boxes over a temporal window.\n",
    "        \"\"\"\n",
    "        for i in range(len(boxes)):\n",
    "            if i + T > len(boxes):\n",
    "                window = boxes[len(boxes) - T:]\n",
    "            else:\n",
    "                window = boxes[i : i + T]\n",
    "            boxes[i] = np.mean(window, axis=0)\n",
    "        return boxes\n",
    "\n",
    "    def face_detect(self, images, args):\n",
    "        \"\"\"\n",
    "        Detect faces in a batch of images using YOLO.\n",
    "        \"\"\"\n",
    "        batch_size = args.face_det_batch_size\n",
    "        # batch_size = 1\n",
    "        \n",
    "        while 1:\n",
    "            predictions = []\n",
    "            try:\n",
    "                for i in range(0, len(images), batch_size):\n",
    "                    results = self.yolo_model.predict(images[i:i + batch_size], verbose=False, device=self.args.device)\n",
    "                    try:\n",
    "                        # boxes = results[0].boxes.xyxy[0].tolist()\n",
    "                        boxes = results[0].boxes.cpu().xyxy[0].tolist()\n",
    "                        predictions.append(boxes)\n",
    "                    except Exception as e:\n",
    "                        cv2.imwrite(f\"temp/faulty_frame{randint(0, 10000)}.jpg\", images[0])\n",
    "                        print(\"face not detected\")\n",
    "                    \n",
    "            except RuntimeError:\n",
    "                if batch_size == 1: \n",
    "                    raise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')\n",
    "                batch_size //= 2\n",
    "                print('Recovering from OOM error; New batch size: {}'.format(batch_size))\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        results = []\n",
    "        pady1, pady2, padx1, padx2 = args.pads\n",
    "        for rect, image in zip(predictions, images):\n",
    "            if rect is None:\n",
    "                cv2.imwrite('temp/faulty_frame.jpg', image) # check this frame where the face was not detected.\n",
    "                raise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n",
    "            \n",
    "            y1 = max(0, int(rect[1]) - pady1)\n",
    "            y2 = min(image.shape[0], int(rect[3]) + pady2)\n",
    "            x1 = max(0, int(rect[0]) - padx1)\n",
    "            x2 = min(image.shape[1], int(rect[2]) + padx2)\n",
    "            \n",
    "            results.append([x1, y1, x2, y2])\n",
    "\n",
    "        boxes = np.array(results)\n",
    "        if not args.nosmooth: \n",
    "            boxes = self.get_smoothened_boxes(boxes, T=5)\n",
    "        results = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n",
    "\n",
    "        return results \n",
    "\n",
    "    def datagen(self, mels, args):\n",
    "        \"\"\"\n",
    "        Data generator for processing batches.\n",
    "        \"\"\"\n",
    "        img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
    "\n",
    "        reader = self.read_frames()\n",
    "        t = []\n",
    "        prev = None\n",
    "        for i, m in enumerate(mels):\n",
    "            try:\n",
    "                frame_to_save = next(reader)\n",
    "            except StopIteration:\n",
    "                reader = self.read_frames()\n",
    "                frame_to_save = next(reader)\n",
    "            \n",
    "            try:\n",
    "                prev = self.face_detect([frame_to_save], args)[0]\n",
    "                face, coords = prev\n",
    "            except:\n",
    "                face, coords = prev\n",
    "\n",
    "            face = cv2.resize(face, (args.img_size, args.img_size))\n",
    "                \n",
    "            if i%10000 == 0:\n",
    "                cv2.imwrite(f\"test{i}.jpg\", face)\n",
    "\n",
    "            img_batch.append(face)\n",
    "            mel_batch.append(m)\n",
    "            frame_batch.append(frame_to_save)\n",
    "            coords_batch.append(coords)\n",
    "\n",
    "            if len(img_batch) >= args.wav2lip_batch_size:\n",
    "                img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
    "\n",
    "                img_masked = img_batch.copy()\n",
    "                img_masked[:, args.img_size//2:] = 0\n",
    "\n",
    "                img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
    "                mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "                yield img_batch, mel_batch, frame_batch, coords_batch\n",
    "                img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
    "        \n",
    "        if len(img_batch) > 0:\n",
    "            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
    "\n",
    "            img_masked = img_batch.copy()\n",
    "            img_masked[:, args.img_size//2:] = 0\n",
    "\n",
    "            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
    "            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "            yield img_batch, mel_batch, frame_batch, coords_batch\n",
    "\n",
    "    def read_frames(self):\n",
    "        \"\"\"\n",
    "        Read frames from a folder of image files.\n",
    "        \"\"\"\n",
    "        \n",
    "        image_files = [f for f in os.listdir(self.args.frame_path) if f.split('.')[-1].lower() in ['jpg', 'png', 'jpeg']]\n",
    "        image_files.sort()\n",
    "\n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(self.args.frame_path, image_file)\n",
    "            frame = cv2.imread(image_path)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            yield frame\n",
    "\n",
    "    def _get_apen_ai_answer(self, prompt, messages):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model = \"gpt-3.5-turbo-1106\",\n",
    "            temperature= 0,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        for chunk in response:\n",
    "            txt = chunk.choices[0].delta.content\n",
    "            # print(txt, end=\"\")\n",
    "            \n",
    "            yield txt if txt != None else \"\"\n",
    "            \n",
    "    def stream(self, audio_stream: Iterator[bytes]) -> bytes:\n",
    "\n",
    "        mpv_command = [\"C:\\\\Program Files\\\\mpv\\\\mpv.exe\", \"--no-cache\", \"--no-terminal\", \"--\", \"fd://0\"]\n",
    "        mpv_process = subprocess.Popen(\n",
    "            mpv_command,\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.DEVNULL,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        )\n",
    "\n",
    "        audio = b\"\"\n",
    "\n",
    "        for chunk in audio_stream:\n",
    "            if chunk is not None:\n",
    "                mpv_process.stdin.write(chunk)  # type: ignore\n",
    "                mpv_process.stdin.flush()  # type: ignore\n",
    "                audio += chunk\n",
    "\n",
    "        if mpv_process.stdin:\n",
    "            mpv_process.stdin.close()\n",
    "        mpv_process.wait()\n",
    "\n",
    "        return audio\n",
    "    \n",
    "    def stream_video(self, video_stream: Iterator[bytes]) -> bytes:\n",
    "\n",
    "        # ffmpeg_command = [\n",
    "        #     'ffmpeg',\n",
    "        #     '-f', 'rawvideo',\n",
    "        #     '-pixel_format', 'bgr24',\n",
    "        #     '-video_size', '720x720',\n",
    "        #     '-i', '-',\n",
    "        #     '-f', 'mpegts',\n",
    "        #     '-codec:v', 'mpeg1video',\n",
    "        #     '-bf', '0',\n",
    "        #     '-'\n",
    "        # ]\n",
    "        # mpv_command = ['|', 'C:\\\\Program Files\\\\mpv\\\\mpv.exe', '--no-cache', '--no-terminal', '--', '-']\n",
    "\n",
    "        # Combine both commands (simplified for explanation; actual implementation may vary)\n",
    "        # process_command = ' '.join(ffmpeg_command + mpv_command)\n",
    "        # process = subprocess.Popen(process_command, stdin=subprocess.PIPE, shell=True)\n",
    "        mpv_command = [\"C:\\\\Program Files\\\\mpv\\\\mpv.exe\", \"--no-cache\", \"--no-terminal\", \"--demuxer=rawvideo\", \"--demuxer-rawvideo-w=720\", \"--demuxer-rawvideo-h=720\", \"--demuxer-rawvideo-fps=25\", \"--\", \"-\"]\n",
    "        # mpv_command = [\n",
    "        #     \"C:\\\\Program Files\\\\mpv\\\\mpv.exe\",\n",
    "        #     \"--no-cache\",\n",
    "        #     \"--no-terminal\",\n",
    "        #     \"temp/res.mp4\"  # Directly specify the video file path here\n",
    "        # ]\n",
    "        # # Replace WIDTH, HEIGHT, and FPS with actual values for your video\n",
    "        mpv_process = subprocess.Popen(\n",
    "            mpv_command,\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.DEVNULL,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        )\n",
    "\n",
    "        video = b\"\"\n",
    "\n",
    "        for chunk in video_stream:\n",
    "            if chunk is not None:\n",
    "                mpv_process.stdin.write(chunk)  # type: ignore\n",
    "                mpv_process.stdin.flush()  # type: ignore\n",
    "                video += chunk\n",
    "\n",
    "        if mpv_process.stdin:\n",
    "            mpv_process.stdin.close()\n",
    "        mpv_process.wait()\n",
    "\n",
    "        return video\n",
    "\n",
    "    def generate_audio(self, prompt, messages):\n",
    "        audio_bytes = elevenlabs.generate(text=self._get_apen_ai_answer(prompt, messages), voice=\"Charlotte\", model=\"eleven_monolingual_v1\", stream=True, )\n",
    "        video_bytes = self._process_video(audio_bytes)\n",
    "        # return\n",
    "        self.stream_frames(video_bytes)\n",
    "        # self.stream(audio_bytes)\n",
    "\n",
    "    def _process_video(self, audio_bytes):\n",
    "        \n",
    "        for chunk in audio_bytes:\n",
    "            if chunk is not None:\n",
    "                self.audio += chunk\n",
    "\n",
    "                audio_segment = AudioSegment.from_file(BytesIO(chunk), format=\"mp3\")\n",
    "                audio_segment.export('temp/temp.mp3', format=\"mp3\")\n",
    "                command = 'ffmpeg -y -i {} -strict -2 {}'.format('temp/temp.mp3', 'temp/temp.wav')\n",
    "                subprocess.call(command, shell=True)\n",
    "                audio_path = 'temp/temp.wav'\n",
    "                wav = audio.load_wav(audio_path, 16000)\n",
    "                mel = audio.melspectrogram(wav)\n",
    "\n",
    "                mel_chunks = []\n",
    "                mel_idx_multiplier = 80./self.args.fps \n",
    "                i = 0\n",
    "                while 1:\n",
    "                    start_idx = int(i * mel_idx_multiplier)\n",
    "                    if start_idx + self.args.mel_step_size > len(mel[0]):\n",
    "                        mel_chunks.append(mel[:, len(mel[0]) - self.args.mel_step_size:])\n",
    "                        break\n",
    "                    mel_chunks.append(mel[:, start_idx : start_idx + self.args.mel_step_size])\n",
    "                    i += 1\n",
    "                print(\"melchuncks: \", len(mel_chunks))\n",
    "                gen = self.datagen(mel_chunks, self.args)\n",
    "                byte_image = b\"\"\n",
    "                for i, (img_batch, mel_batch, frames, coords) in enumerate(gen):\n",
    "                    img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(self.args.device)\n",
    "                    mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(self.args.device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        try:\n",
    "                            pred = self.wav2lib_model(mel_batch, img_batch)\n",
    "                        except:\n",
    "                            print(1)\n",
    "                            f = cv2.imread(\"frames/0.jpg\")\n",
    "                            self.out.write(f)\n",
    "                            continue\n",
    "\n",
    "                    pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
    "                    \n",
    "                    for p, f, c in zip(pred, frames, coords):\n",
    "                        y1, y2, x1, x2 = c\n",
    "                        p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n",
    "                        f[y1:y2, x1:x2] = p\n",
    "                        # self.out.write(f) \n",
    "\n",
    "                        frame_bytes = cv2.imencode('.jpg', f)[1].tobytes()\n",
    "\n",
    "                        yield frame_bytes\n",
    "                        # _, encoded_image = cv2.imencode('.png', f) \n",
    "                        # byte_image += encoded_image.tobytes()\n",
    "            else:\n",
    "                print(1)\n",
    "                f = cv2.imread(\"frames/0.jpg\")\n",
    "                frame_bytes = cv2.imencode('.jpg', f)[1].tobytes()\n",
    "                yield frame_bytes\n",
    "                # yield byte_image               \n",
    "                    \n",
    "    def stream_frames(self, frame_generator):\n",
    "       \n",
    "        for frame_bytes in frame_generator:\n",
    "            nparr = np.frombuffer(frame_bytes, np.uint8)\n",
    "            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            self.out.write(img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from: E:\\chat\\wav2lip_master\\checkpoints\\wav2lip.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12/12 [00:02<00:00,  4.78it/s]\n"
     ]
    }
   ],
   "source": [
    "answer_me = Casy(\"Master Machine Learning Algorithms - Discover how they work by Jason Brownlee (z-lib.org).docx\", 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melchuncks:  36\n",
      "melchuncks:  40\n",
      "melchuncks:  8\n",
      "melchuncks:  40\n",
      "melchuncks:  1\n",
      "1\n",
      "melchuncks:  40\n",
      "melchuncks:  35\n",
      "melchuncks:  40\n",
      "melchuncks:  1\n",
      "1\n",
      "melchuncks:  40\n",
      "melchuncks:  27\n",
      "melchuncks:  41\n",
      "melchuncks:  39\n",
      "melchuncks:  34\n",
      "melchuncks:  39\n",
      "melchuncks:  6\n",
      "melchuncks:  22\n",
      "melchuncks:  12\n"
     ]
    }
   ],
   "source": [
    "answer_me.run(\"explain linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    txt = input(\"\")\n",
    "    \n",
    "    if txt == \"End Session\": break\n",
    "    answer_me.run(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading video frames from start...\n"
     ]
    }
   ],
   "source": [
    "vidcap = cv2.VideoCapture(\"inputs/ms.mp4\")\n",
    "total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('Reading video frames from start...')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for frameNumber in range(total_frames):\n",
    "    _, frame = vidcap.read()\n",
    "\n",
    "    cv2.imwrite(f\"frames/{i}.jpg\", frame)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpv_command = [\n",
    "    \"C:\\\\Program Files\\\\mpv\\\\mpv.exe\",\n",
    "    \"--no-cache\",\n",
    "    \"--no-terminal\",\n",
    "    \"temp/res.mp4\"  # Directly specify the video file path here\n",
    "]\n",
    "# Replace WIDTH, HEIGHT, and FPS with actual values for your video\n",
    "mpv_process = subprocess.Popen(\n",
    "    mpv_command,\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "\n",
    "# video = b\"\"\n",
    "\n",
    "# for chunk in video_stream:\n",
    "#     if chunk is not None:\n",
    "#         mpv_process.stdin.write(chunk)  # type: ignore\n",
    "#         mpv_process.stdin.flush()  # type: ignore\n",
    "#         video += chunk\n",
    "\n",
    "# if mpv_process.stdin:\n",
    "#     mpv_process.stdin.close()\n",
    "mpv_process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misallam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
